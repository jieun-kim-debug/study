# 다층 퍼셉트론은 입력층 하나와 은닉층 하나 이상의 TLU층과 마지막 출력층으로 구성
# 모든 층은 편향 뉴런을 포함하며 다음 층과 완전히 연결
# 역전파 훈련 알고리즘 사용 : 그레디언트를 자동으로 계산하는 경사 하강법(정방향 한번, 역방향 한번) 모든 모델 파라미터에 대한 네트워크 오차의 그레디언트 계산
# 자동으로 그레디언트를 계산하는 것을 자동 미분. 역전파에서 사용하는 기법은 후진 모드 자동 미분
# 미분할 함수가 변수(ex.연결 가중치)가 많고 출력이 적은 경우 잘 맞음

# 요약하면, 
#1) 역전파 알고리즘이 먼저 예측(정방향 계산)을 만들고 오차 측정
#2) 역방향으로 각 층을 거치면서 각 연결이 오차에 기여한 정도 측정
#3) 이 오차가 감소하도록 가중치 조정(경사하강법)

# 왜 활성화 함수가 필요할까? 선형 변환을 여러개 연결해도 얻을 수 있는 것은 선형 변환뿐
#ex) 두 선형 함수 f(x)=2x+3과 g(x)=5x-1을 연결하면 f(g(x))=2(5x-1)=10x+1이 되므로
# 따라서 층 사이에 비선형성을 추가하지 않으면 아무리 층을 많이 쌓아도 하나의 층과 동일
