# 활성화 함수 : 주로 전결합층 뒤에 적용하는 함수로 뉴런의 발화에 해당
# 전결합층에서는 입력을 선형 변환한것을 출력하지만 활성화 함수를 이용하면 비선형성을 가지게 됨
# 활성화함수를 사용하지 않을 경우 단일 직선으로 분리할 수 없는 데이터는 분류가 불가능
# 비선형을 갖게하여 학습이 진행되면 선형 분리 불가능한 모델도 분류 가능

# 시그모이드 함수
# 출력은 반드시 구간 (0,1) 안에 들어가기 때문에 극단적인 출력값이 적다

# Relu 함수
# Rectified Linear Unit의 약자
# relu(x) = 0(x<0), x(x>=0)
# 출력은 어떤 구간에도 수렴되지 않고, 극단적인 출력값이 생성될 가능성이 있다
